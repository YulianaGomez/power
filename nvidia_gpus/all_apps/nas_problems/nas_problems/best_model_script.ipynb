{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def get_best_model_path(path_to_data_folder):\n",
    "\n",
    "    width = 21\n",
    "    height = width/1.618\n",
    "\n",
    "    def load_json(path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(path_to_data_folder):\n",
    "        if 'task' in root.split('/')[-1]:\n",
    "\n",
    "            rank = int(root.split('/')[-1].split('_')[0].split('task')[-1])\n",
    "            try:\n",
    "                th_fn = list(filter(lambda name: 'training_hist' in name, files))[0]\n",
    "            except:\n",
    "                print(f'folder: {root.split(\"/\")[-1]} doesn\\'t contain a training history file.')\n",
    "                print('   files: ', files)\n",
    "                continue\n",
    "            data[rank] = load_json(os.path.join(root, th_fn))\n",
    "\n",
    "    data = {k:v for k,v in data.items() if 'val_r2' in v}\n",
    "    ranks = list(data.keys())\n",
    "    ranks.sort()\n",
    "    metrics_names = list(data[list(data.keys())[0]].keys())\n",
    "\n",
    "    mn = 'val_r2'\n",
    "    best_rank = None\n",
    "    best_mn = -1\n",
    "    limit=100\n",
    "\n",
    "    for rank in ranks:\n",
    "        if rank >= limit:\n",
    "            continue\n",
    "        max_mn = max(data[rank][mn])\n",
    "        if max_mn > best_mn:\n",
    "            best_mn = max_mn\n",
    "            best_rank = rank\n",
    "\n",
    "    best_model_path = glob.glob(path_to_data_folder + \"/task\" + str(best_rank) + \"_*\")\n",
    "    if len(best_model_path) > 1:\n",
    "        print(\"WARNING -- MORE THAN ONE DIRECTORY FOR THIS CASE!!\")\n",
    "    \n",
    "    best_model_path = glob.glob(best_model_path[0]+\"/*.h5\")\n",
    "    if len(best_model_path) > 1:\n",
    "        print(\"WARNING -- MORE THAN ONE MODEL FOR THIS CASE!!\")\n",
    "    best_model_path = best_model_path[0]\n",
    "\n",
    "    print(\"Best-Model File Name:\", os.path.basename(best_model_path))\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lus/theta-fs0/projects/datascience/yzamora/AL_RM_20'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-Model File Name: best_model_12.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/yzamora/datascience_project/data_base/gpudb/data/AL_RM_post_results/AL_RM_20-POST/task12_3fbc55e3/best_model_12.h5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test `get_best_model_path` function for single input\n",
    "\n",
    "input_path = '/home/yzamora/datascience_project/data_base/gpudb/data/AL_RM_post_results/AL_RM_20-POST'\n",
    "\n",
    "get_best_model_path(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-Model File Name: best_model_1.h5\n",
      "Best-Model File Name: best_model_9.h5\n",
      "Best-Model File Name: best_model_26.h5\n",
      "Best-Model File Name: best_model_37.h5\n",
      "Best-Model File Name: best_model_22.h5\n",
      "Best-Model File Name: best_model_10.h5\n",
      "Best-Model File Name: best_model_45.h5\n",
      "Best-Model File Name: best_model_6.h5\n",
      "Best-Model File Name: best_model_18.h5\n",
      "Best-Model File Name: best_model_4.h5\n",
      "Best-Model File Name: best_model_20.h5\n",
      "Best-Model File Name: best_model_12.h5\n",
      "Best-Model File Name: best_model_14.h5\n",
      "Best-Model File Name: best_model_49.h5\n",
      "Best-Model File Name: best_model_8.h5\n",
      "Best-Model File Name: best_model_0.h5\n",
      "Best-Model File Name: best_model_25.h5\n",
      "Best-Model File Name: best_model_7.h5\n",
      "Best-Model File Name: best_model_6.h5\n",
      "Best-Model File Name: best_model_18.h5\n",
      "Best-Model File Name: best_model_7.h5\n",
      "Best-Model File Name: best_model_33.h5\n",
      "Best-Model File Name: best_model_12.h5\n",
      "Best-Model File Name: best_model_43.h5\n",
      "Best-Model File Name: best_model_22.h5\n",
      "Best-Model File Name: best_model_40.h5\n",
      "Best-Model File Name: best_model_0.h5\n",
      "Best-Model File Name: best_model_49.h5\n",
      "Best-Model File Name: best_model_38.h5\n",
      "Best-Model File Name: best_model_4.h5\n",
      "Best-Model File Name: best_model_11.h5\n",
      "Best-Model File Name: best_model_25.h5\n",
      "Best-Model File Name: best_model_36.h5\n",
      "Best-Model File Name: best_model_12.h5\n",
      "Best-Model File Name: best_model_10.h5\n",
      "Best-Model File Name: best_model_48.h5\n"
     ]
    }
   ],
   "source": [
    "root_path = '/home/yzamora/datascience_project/data_base/gpudb/data/'\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for path in glob.glob(root_path + \"/*post*\"):\n",
    "    for path_2 in glob.glob(path + \"/*\"):\n",
    "        label = os.path.basename(path_2)\n",
    "        best_models[label] = get_best_model_path(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = root_path + \"best_models\"\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, path in best_models.items():\n",
    "    new_path = new_dir + \"/\" + label + \"_best.h5\"\n",
    "    cmd = \"cp \" + path + \" \" + new_path\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL_ALL_10-POST_best.h5\tAL_one_13-POST_best.h5\tAL_RM_20-POST_best.h5\r\n",
      "AL_ALL_13-POST_best.h5\tAL_one_15-POST_best.h5\tAL_RM_23-POST_best.h5\r\n",
      "AL_ALL_15-POST_best.h5\tAL_one_18-POST_best.h5\tAL_RM_25-POST_best.h5\r\n",
      "AL_ALL_18-POST_best.h5\tAL_one_20-POST_best.h5\tAL_RM_28-POST_best.h5\r\n",
      "AL_ALL_20-POST_best.h5\tAL_one_30-POST_best.h5\tAL_RM_30-POST_best.h5\r\n",
      "AL_ALL_23-POST_best.h5\tAL_one_3-POST_best.h5\tAL_RM_3-POST_best.h5\r\n",
      "AL_ALL_25-POST_best.h5\tAL_one_5-POST_best.h5\tAL_RM_5-POST_best.h5\r\n",
      "AL_ALL_28-POST_best.h5\tAL_one_8-POST_best.h5\tAL_RM_8-POST_best.h5\r\n",
      "AL_ALL_30-POST_best.h5\tAL_RM_10-POST_best.h5\tRANDOM_ALL_10-POST_best.h5\r\n",
      "AL_ALL_5-POST_best.h5\tAL_RM_13-POST_best.h5\tRANDOM_ALL_13-POST_best.h5\r\n",
      "AL_ALL_8-POST_best.h5\tAL_RM_15-POST_best.h5\tRANDOM_ALL_15-POST_best.h5\r\n",
      "AL_one_10-POST_best.h5\tAL_RM_18-POST_best.h5\tRANDOM_ALL_18-POST_best.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/yzamora/datascience_project/data_base/gpudb/data/best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deephyper)",
   "language": "python",
   "name": "deephyper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
