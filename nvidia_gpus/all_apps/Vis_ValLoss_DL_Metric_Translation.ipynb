{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, \\\n",
    "    GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing random forest model and libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy as sp\n",
    "import pickle as pkl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if True:\n",
    "    df_P100 = pd.read_csv('p100_all_data.csv', index_col = 0)\n",
    "    df_V100 = pd.read_csv('v100_all_data.csv', index_col = 0)\n",
    "    new_df = pd.read_csv('new_df.csv', index_col = 0)\n",
    "    all_data = pd.read_csv('all_data.csv', index_col = 0)\n",
    "if True:\n",
    "    all_data = all_data.dropna(axis=1,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30622, 120)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['dram_through']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new_df = P100 Metrics with V100 IPC\n",
    "df_new = new_df\n",
    "\n",
    "#grabbing IPC values\n",
    "newdf_ipc = df_new.drop(columns=['shared_utilization','stall_other','single_precision_fu_utilization','architecture','input','application_name','kernelname'])\n",
    "newdf_ipc = newdf_ipc['ipc']\n",
    "new_ipc_values = newdf_ipc.values\n",
    "\n",
    "df_new_norm = df_new.drop(columns=['shared_utilization','stall_other','single_precision_fu_utilization','architecture','input','application_name','kernelname','ipc'])\n",
    "df_new_norm = df_new_norm.values\n",
    "df_new_norm = MinMaxScaler().fit_transform(df_new_norm)\n",
    "\n",
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_new = df_new_norm\n",
    "Y_new = new_ipc_values\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_trainnew, X_testnew, y_trainnew, y_testnew = train_test_split(X_new, Y_new, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Creates array of ipc values and array of metrics \n",
    "def process(new_df,test_size):\n",
    "    ipc_values = new_df['ipc'].values\n",
    "    new_def_norm = new_df.drop(columns=['shared_utilization','stall_other','single_precision_fu_utilization','architecture','input','ipc','application_name','kernelname'])\n",
    "\n",
    "    #v100+p100 combined\n",
    "    new_def_norm_values = new_def_norm.values\n",
    "    new_def_norm = MinMaxScaler().fit_transform(new_def_norm_values)\n",
    "\n",
    "    ###!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!####\n",
    "    X_VP = new_def_norm \n",
    "    Y_VP = ipc_values\n",
    "\n",
    "    # Split the data up in train and test sets\n",
    "    X_trainVP, X_testVP, y_trainVP, y_testVP = train_test_split(X_VP, Y_VP, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Import `StandardScaler` from `sklearn.preprocessing`\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Define the scaler \n",
    "    #scalerVP = StandardScaler().fit(X_trainVP)\n",
    "    scalerVP = StandardScaler().fit(X_trainVP)\n",
    "\n",
    "    # Scale the train set\n",
    "    X_trainVP = scalerVP.transform(X_trainVP)\n",
    "\n",
    "    # Scale the test set\n",
    "    X_testVP = scalerVP.transform(X_testVP)\n",
    "    \n",
    "    return X_trainVP, X_testVP, y_trainVP, y_testVP, X_VP, Y_VP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Creates array of ipc values and array of metrics \n",
    "def process_keep(new_df):\n",
    "    #new_def_norm = new_df.drop(columns=['shared_utilization','stall_other','single_precision_fu_utilization','architecture','input','ipc','application_name','kernelname'])\n",
    "    new_def_norm = new_df.drop(columns=['architecture','input','application_name','kernelname'])\n",
    "\n",
    "    #v100+p100 combined\n",
    "    new_def_norm_values = new_def_norm.values\n",
    "    ##new_def_norm = MinMaxScaler().fit_transform(new_def_norm_values)\n",
    "\n",
    "    ###!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!####\n",
    "    ##X_VP = new_def_norm \n",
    "\n",
    "    # NOT SPLITTING HERE - Split the data up in train and test sets\n",
    "    ##X_trainVP, X_testVP, y_trainVP, y_testVP = train_test_split(X_VP, Y_VP, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Import `StandardScaler` from `sklearn.preprocessing`\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Define the scaler \n",
    "    #scalerVP = StandardScaler().fit(X_trainVP)\n",
    "    ##scalerVP = StandardScaler().fit(X_VP)\n",
    "    scalerVP = StandardScaler().fit(new_def_norm_values)\n",
    "\n",
    "    # Scale the train set\n",
    "    ##X_trainVP = scalerVP.transform(X_VP)\n",
    "    X_trainVP = scalerVP.transform(new_def_norm_values)\n",
    "\n",
    "    # Scale the test set\n",
    "    #X_testVP = scalerVP.transform(X_testVP)\n",
    "    \n",
    "    return X_trainVP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating two data sets \n",
    "from_scratch = False\n",
    "df = pd.read_csv('all_data.csv', index_col = 0)\n",
    "df = df.dropna(axis=1,how='any')\n",
    "if from_scratch:\n",
    "    df_check = None\n",
    "    frames = []\n",
    "    frame2 = []\n",
    "    df_v = []\n",
    "    df_p = []\n",
    "    for name, item in df.groupby(['kernelname', 'input']):\n",
    "        df_1 = df[ df['kernelname'] == name[0] ]\n",
    "        df_2 = df_1[ df_1['input'] == name[1] ]\n",
    "        if len(df_2) == 2:\n",
    "            df_v = df_2[df_2['architecture'] == 'V100']\n",
    "            df_p = df_2[df_2['architecture'] == 'P100']\n",
    "            #df_p['ipc'][0] = df_v.iloc[0]['ipc']\n",
    "            frames.append(df_p)\n",
    "            frame2.append(df_v)\n",
    "\n",
    "    new_df = pd.concat(frames)\n",
    "    new_v = pd.concat(frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_df.shape)\n",
    "#print(new_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START HERE\n",
    "from_scratch = False\n",
    "#####!!!!! start here !!! new data analysis!!!####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if from_scratch:\n",
    "    new_df.to_csv('cor_p100.csv')\n",
    "    new_v.to_csv('cor_v100.csv')\n",
    "else:\n",
    "    #new_p = pd.read_csv('cor_p100.csv', index_col = 0)\n",
    "    #new_v = pd.read_csv('cor_v100.csv', index_col = 0)\n",
    "    #new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/spec_p100.csv', index_col = 0) #using specified kernels\n",
    "    #new_v = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/spec_v100.csv', index_col = 0) #using specified kernels\n",
    "    new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_P100_10Per.csv', index_col = 0)\n",
    "    new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_V100_10Per.csv', index_col = 0)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_p.shape)\n",
    "#print(new_v.shape)\n",
    "# Need to get correct data shape + set from v100_spec_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_trainP, X_testP, y_trainP, y_testP, X_P, Y_P = process(new_p,.2)\n",
    "#X_trainV, X_testV, y_trainV, y_testV, X_V, Y_V = process(new_v,.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weighted mse\n",
    "import keras.backend as K\n",
    "def weighted_mse(loss_weight):\n",
    "    def loss(y_true, y_pred):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #y_true = K.variable(y_true)\n",
    "        #y_pred = K.variable(y_pred)\n",
    "        #loss = K.mean(K.dot(K.square(y_true - y_pred),loss_weight))\n",
    "        loss = K.mean(K.square(y_true - y_pred)*loss_weight)\n",
    "        return loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "#in model.compile: loss=weighted_mse(loss_weight) instead of loss='mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipc = 96, dram_read_throughput = 33,dram_write_throughput = 34,  ldst_fu_utilization=105\n",
    "#cf_fu_utilization = 106, \n",
    "#loss_weight = np.ones((116,1)) ##setting model to zero on those two items and see how it changes\n",
    "loss_weight = np.zeros(116)\n",
    "loss_weight[96] = 0#17\n",
    "loss_weight[33] = 100#15 ## increasing dram write and read weights\n",
    "loss_weight[34] = 100#14\n",
    "loss_weight[105] = 0#13\n",
    "loss_weight[106] = 0#10\n",
    "#print(loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. how does p100 ipc to predict v100 ipc (fit a line)\\n\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need baselines\n",
    "\"\"\"\n",
    "1. how does p100 ipc to predict v100 ipc (fit a line)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import regularizers\n",
    "\n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "#dataset = pd.read_csv(\"p100_only.csv\", delimiter=\",\").values\n",
    "# split into input (X) and output (Y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#Y = dataset[:,8]\n",
    "# create model\n",
    "def my_model(l2_weight):\n",
    "    model = Sequential()\n",
    "    \"\"\"\n",
    "    #original simple dl model\n",
    "    model.add(Dense(12, input_dim=112, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\"\"\"\n",
    "    #early stopping, smaller layers, less layers\n",
    "    model.add(Dense(130, input_dim=116, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l2(l2_weight)))\n",
    "    model.add(Dense(125, activation='relu',kernel_regularizer=regularizers.l2(l2_weight)))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(90, activation='relu'))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(110, activation='relu'))\n",
    "    #rerun and check everything is normalized correctly\n",
    "    model.add(Dense(125, activation='relu', kernel_regularizer=regularizers.l2(l2_weight)))\n",
    "    model.add(Dense(116, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_weight)))\n",
    "    # Compile model\n",
    "    #mean absolute percentage error - indicating that we seek to minimize the mean percentage difference between \n",
    "    #predicted ipc and the actual ipc\n",
    "    #loss_weight = vector of weights \n",
    "    #model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    model.compile(loss=weighted_mse(loss_weight),optimizer='adam',metrics=['mse','mae'])\n",
    "    # Fit the model\n",
    "    ## model.fit(X, Y, epochs=10, batch_size=10) ##works\n",
    "    #look at weighted mean square error - putting more weight on certain metrics \n",
    "    \"\"\"\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X, Y)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\"\"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNvidia gives a rough classifier - below and above threshold\\n- which applications would change class - 'from memory bound to compute bound'\\n- tool where to focus benchmarking applications\\n- make sure to get baseline\\n\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###NOTES\n",
    "\"\"\"\n",
    "Nvidia gives a rough classifier - below and above threshold\n",
    "- which applications would change class - 'from memory bound to compute bound'\n",
    "- tool where to focus benchmarking applications\n",
    "- make sure to get baseline\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.callbacks import EarlyStopping\\nfrom keras.callbacks import ModelCheckpoint\\n\\n#early stopping - once validation error stops improving, cut it off\\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, verbose=1, mode='min')\\ncallbacks=earlystop\\np_model = my_model()\\nmc = ModelCheckpoint('filename.h5', monitor='val_loss', mode='min', verbose=1,save_best_only=True)\\n#callbacks_list = [earlystop, mc]\\np_model.fit(X_trainP, X_trainV, epochs=2000, batch_size=10000, verbose=1,validation_split=0.7,callbacks=[earlystop,mc])\\n\\n\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#early stopping - once validation error stops improving, cut it off\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, verbose=1, mode='min')\n",
    "callbacks=earlystop\n",
    "p_model = my_model()\n",
    "mc = ModelCheckpoint('filename.h5', monitor='val_loss', mode='min', verbose=1,save_best_only=True)\n",
    "#callbacks_list = [earlystop, mc]\n",
    "p_model.fit(X_trainP, X_trainV, epochs=2000, batch_size=10000, verbose=1,validation_split=0.7,callbacks=[earlystop,mc])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specified validation sets\n",
    "XV_val = np.load('XV_val_data_all_metrics.npy')\n",
    "XP_val = np.load('XP_val_data_all_metrics.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening columns\n",
    "XP_column_names = np.load('column_names.npy')\n",
    "column_name_dict = {}\n",
    "for i,name in enumerate(XP_column_names):\n",
    "    column_name_dict[name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116,)\n",
      "34\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(XP_column_names.shape)\n",
    "#print(XP_column_names)\n",
    "print(column_name_dict['dram_write_throughput'])\n",
    "print(column_name_dict['dram_read_throughput'])\n",
    "\n",
    "#XP_val[:,23]\n",
    "#ipc = 96, dram_read_throughput = 33,dram_write_throughput = 34,  ldst_fu_utilization=105\n",
    "#cf_fu_utilization = 106, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of weights for the specified dataset\n",
    "# indices (`indices`), given the parent dataframe (`df`)\n",
    "# and a dictionary of weights to apply. Application names\n",
    "# that are not included in `weights` will be set to 1.0\n",
    "# does my model put the points in the right region in comparison to the truth values\n",
    "# you know will be higher for these applications on V100 than P100 - can identify which applications this happens\n",
    "# !!!!!is there a point where it becomes accurate enough to be useful?   AM_AL_spec_P100_10Per.csv\n",
    "##new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_P100_20per.csv', index_col = 0) #using specified kernels\n",
    "new_p = pd.read_csv('/Users/yzamora/power/nvidia_gpus/all_apps/predicted_true_p100.csv')\n",
    "    \n",
    "def get_w_vec(df, weights, indices=None):\n",
    "    w = []\n",
    "    indices = indices or [i for i in range(len(df))]\n",
    "    for ind in indices:\n",
    "        default = 1.0\n",
    "        name = df.iloc[ind][\"application_name\"]\n",
    "        w.append(weights.get(name, default))\n",
    "    return w\n",
    "\n",
    "train_weights = get_w_vec(\n",
    "    new_p,\n",
    "    {\n",
    "        \"backprop\": 1, #-20.0,\n",
    "        \"stream\": 1, #10000.0,\n",
    "        \"leuokocyte\": 1, #10000.0,\n",
    "        \"hybridsort\": 1000, #10000.0,\n",
    "        \"kmeans\": 1.0\n",
    "    },\n",
    ")  \n",
    "\n",
    "#len(new_p)\n",
    "#train_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_P100_20per.csv', index_col = 0) #\n",
    "#new_p['kernelname']\n",
    "#new_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with various training data sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "all_data = False\n",
    "p100_p100 = False\n",
    "p100_v100 = True\n",
    "def testing_training_size(regularization):\n",
    "    #training size decreases\n",
    "    if all_data:\n",
    "        new_p = pd.read_csv('cor_p100.csv', index_col = 0)\n",
    "        new_v = pd.read_csv('cor_v100.csv', index_col = 0)\n",
    "    else: \n",
    "        ##new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_P100_20per.csv', index_col = 0) #using specified kernels\n",
    "        ##new_v = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_V100_20per.csv', index_col = 0)\n",
    "        new_p = pd.read_csv('/Users/yzamora/power/nvidia_gpus/all_apps/predicted_true_p100.csv') #using specified kernels\n",
    "        new_v = pd.read_csv('/Users/yzamora/power/nvidia_gpus/all_apps/predicted_true_v100.csv')\n",
    "        \n",
    "        #X_trainP, X_testP, y_trainP, y_testP, X_P, Y_P = process(new_p,test_size)\n",
    "        #X_trainV, X_testV, y_trainV, y_testV, X_V, Y_V = process(new_v,test_size)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #need to preprocess data with splitting it\n",
    "        X_trainP = process_keep(new_p)\n",
    "        X_trainV = process_keep(new_v)\n",
    "\n",
    "        h_model = my_model(regularization)\n",
    "        #pass in validation data - remove validation split, do not split training set again\n",
    "        #history = h_model.fit(X_trainP, X_trainV, epochs=100, batch_size=500,  verbose=1, validation_split=0.2)\n",
    "        earlystop = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, verbose=1, mode='min')\n",
    "        mc = ModelCheckpoint('membounds_dram_kmeans_zerobias.h5', monitor='val_loss', mode='min', verbose=1,save_best_only=True)\n",
    "        #h_model.save('wsame_PV_dl_10per_woutbias' + str(int(regularization)) +'.h5')\n",
    "        #create a separate vector with a weight for the applications - put this vector into sample weight, after callbacks\n",
    "        #sample_weight = \n",
    "        history = h_model.fit(X_trainP, X_trainV, epochs=1000, batch_size=50,  verbose=1, validation_data=(XP_val,XV_val),callbacks=[earlystop,mc],sample_weight=np.array(train_weights))\n",
    "        #h_model.save('wAsame_PV_dl_10per_woutbias' + str(int(regularization)) +'.h5')\n",
    "        return history.history['val_loss']\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2534, 120)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_p = pd.read_csv('/Users/yzamora/Desktop/ActiveLearningFrameworkTutorial/with_same_base_P100/AM_AL_spec_P100_10per.csv', index_col = 0)\n",
    "#new_p['dram_utilization']\n",
    "new_v = pd.read_csv('/Users/yzamora/power/nvidia_gpus/all_apps/predicted_true_v100.csv')\n",
    "new_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over sampling certain datapoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2534 samples, validate on 4116 samples\n",
      "Epoch 1/1000\n",
      "2534/2534 [==============================] - 4s 2ms/step - loss: 1.8563 - mean_squared_error: 0.9634 - mean_absolute_error: 0.5744 - val_loss: 0.3022 - val_mean_squared_error: 0.2159 - val_mean_absolute_error: 0.4071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30220, saving model to membounds_dram_kmeans_zerobias.h5\n",
      "Epoch 2/1000\n",
      "2534/2534 [==============================] - 0s 142us/step - loss: 1.7135 - mean_squared_error: 0.9729 - mean_absolute_error: 0.5736 - val_loss: 0.3020 - val_mean_squared_error: 0.1983 - val_mean_absolute_error: 0.4024\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30220 to 0.30198, saving model to membounds_dram_kmeans_zerobias.h5\n",
      "Epoch 3/1000\n",
      "2534/2534 [==============================] - 0s 148us/step - loss: 1.6557 - mean_squared_error: 0.9634 - mean_absolute_error: 0.5708 - val_loss: 0.3034 - val_mean_squared_error: 0.1932 - val_mean_absolute_error: 0.4004\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30198\n",
      "Epoch 4/1000\n",
      "2534/2534 [==============================] - 0s 152us/step - loss: 1.6278 - mean_squared_error: 0.9554 - mean_absolute_error: 0.5692 - val_loss: 0.3036 - val_mean_squared_error: 0.1878 - val_mean_absolute_error: 0.4001\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30198\n",
      "Epoch 5/1000\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 1.6313 - mean_squared_error: 0.9523 - mean_absolute_error: 0.5691 - val_loss: 0.3038 - val_mean_squared_error: 0.1853 - val_mean_absolute_error: 0.4010\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30198\n",
      "Epoch 6/1000\n",
      "2534/2534 [==============================] - 0s 150us/step - loss: 1.6284 - mean_squared_error: 0.9511 - mean_absolute_error: 0.5691 - val_loss: 0.3038 - val_mean_squared_error: 0.1846 - val_mean_absolute_error: 0.4022\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30198\n",
      "Epoch 7/1000\n",
      "2534/2534 [==============================] - 0s 151us/step - loss: 1.6272 - mean_squared_error: 0.9508 - mean_absolute_error: 0.5694 - val_loss: 0.3038 - val_mean_squared_error: 0.1847 - val_mean_absolute_error: 0.4030\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30198\n",
      "Epoch 8/1000\n",
      "2534/2534 [==============================] - 0s 157us/step - loss: 1.6270 - mean_squared_error: 0.9509 - mean_absolute_error: 0.5697 - val_loss: 0.3040 - val_mean_squared_error: 0.1850 - val_mean_absolute_error: 0.4035\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30198\n",
      "Epoch 9/1000\n",
      "2534/2534 [==============================] - 1s 216us/step - loss: 1.6160 - mean_squared_error: 0.9511 - mean_absolute_error: 0.5699 - val_loss: 0.3039 - val_mean_squared_error: 0.1852 - val_mean_absolute_error: 0.4038\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30198\n",
      "Epoch 10/1000\n",
      "2534/2534 [==============================] - 1s 239us/step - loss: 1.6159 - mean_squared_error: 0.9513 - mean_absolute_error: 0.5701 - val_loss: 0.3039 - val_mean_squared_error: 0.1854 - val_mean_absolute_error: 0.4040\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.30198\n",
      "Epoch 11/1000\n",
      "2534/2534 [==============================] - 0s 180us/step - loss: 1.6157 - mean_squared_error: 0.9514 - mean_absolute_error: 0.5701 - val_loss: 0.3037 - val_mean_squared_error: 0.1854 - val_mean_absolute_error: 0.4040\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.30198\n",
      "Epoch 12/1000\n",
      "2534/2534 [==============================] - 0s 178us/step - loss: 1.6156 - mean_squared_error: 0.9514 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.30198\n",
      "Epoch 13/1000\n",
      "2534/2534 [==============================] - 1s 228us/step - loss: 1.6156 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.30198\n",
      "Epoch 14/1000\n",
      "2534/2534 [==============================] - 0s 193us/step - loss: 1.6155 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.30198\n",
      "Epoch 15/1000\n",
      "2534/2534 [==============================] - 0s 142us/step - loss: 1.6155 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.30198\n",
      "Epoch 16/1000\n",
      "2534/2534 [==============================] - 0s 151us/step - loss: 1.6157 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.30198\n",
      "Epoch 17/1000\n",
      "2534/2534 [==============================] - 0s 137us/step - loss: 1.6154 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30198\n",
      "Epoch 18/1000\n",
      "2534/2534 [==============================] - 0s 141us/step - loss: 1.6156 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30198\n",
      "Epoch 19/1000\n",
      "2534/2534 [==============================] - 0s 144us/step - loss: 1.6153 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30198\n",
      "Epoch 20/1000\n",
      "2534/2534 [==============================] - 0s 143us/step - loss: 1.6154 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3038 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30198\n",
      "Epoch 21/1000\n",
      "2534/2534 [==============================] - 0s 144us/step - loss: 1.6157 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3037 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.30198\n",
      "Epoch 22/1000\n",
      "2534/2534 [==============================] - 0s 155us/step - loss: 1.6154 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.30198\n",
      "Epoch 23/1000\n",
      "2534/2534 [==============================] - 0s 159us/step - loss: 1.6154 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.30198\n",
      "Epoch 24/1000\n",
      "2534/2534 [==============================] - 0s 135us/step - loss: 1.6152 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3038 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.30198\n",
      "Epoch 25/1000\n",
      "2534/2534 [==============================] - 0s 136us/step - loss: 1.6153 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30198\n",
      "Epoch 26/1000\n",
      "2534/2534 [==============================] - 0s 137us/step - loss: 1.6152 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30198\n",
      "Epoch 27/1000\n",
      "2534/2534 [==============================] - 0s 139us/step - loss: 1.6152 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30198\n",
      "Epoch 28/1000\n",
      "2534/2534 [==============================] - 0s 172us/step - loss: 1.6151 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30198\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 1s 227us/step - loss: 1.6152 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30198\n",
      "Epoch 30/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.30198\n",
      "Epoch 31/1000\n",
      "2534/2534 [==============================] - 0s 196us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.30198\n",
      "Epoch 32/1000\n",
      "2534/2534 [==============================] - 1s 207us/step - loss: 1.6153 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30198\n",
      "Epoch 33/1000\n",
      "2534/2534 [==============================] - 1s 215us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.30198\n",
      "Epoch 34/1000\n",
      "2534/2534 [==============================] - 0s 152us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30198\n",
      "Epoch 35/1000\n",
      "2534/2534 [==============================] - 0s 168us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.30198\n",
      "Epoch 36/1000\n",
      "2534/2534 [==============================] - 0s 151us/step - loss: 1.6150 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.30198\n",
      "Epoch 37/1000\n",
      "2534/2534 [==============================] - 0s 145us/step - loss: 1.6152 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.30198\n",
      "Epoch 38/1000\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 1.6151 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.30198\n",
      "Epoch 39/1000\n",
      "2534/2534 [==============================] - 0s 153us/step - loss: 1.6149 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.30198\n",
      "Epoch 40/1000\n",
      "2534/2534 [==============================] - 0s 150us/step - loss: 1.6147 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.30198\n",
      "Epoch 41/1000\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 1.6147 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.30198\n",
      "Epoch 42/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6148 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.30198\n",
      "Epoch 43/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6148 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30198\n",
      "Epoch 44/1000\n",
      "2534/2534 [==============================] - 0s 150us/step - loss: 1.6148 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.30198\n",
      "Epoch 45/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6149 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.30198\n",
      "Epoch 46/1000\n",
      "2534/2534 [==============================] - 0s 149us/step - loss: 1.6147 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3036 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.30198\n",
      "Epoch 47/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6148 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.30198\n",
      "Epoch 48/1000\n",
      "2534/2534 [==============================] - 0s 149us/step - loss: 1.6146 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.30198\n",
      "Epoch 49/1000\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 1.6146 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.30198\n",
      "Epoch 50/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6147 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.30198\n",
      "Epoch 51/1000\n",
      "2534/2534 [==============================] - 0s 147us/step - loss: 1.6146 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.30198\n",
      "Epoch 52/1000\n",
      "2534/2534 [==============================] - 0s 164us/step - loss: 1.6144 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.30198\n",
      "Epoch 53/1000\n",
      "2534/2534 [==============================] - 0s 164us/step - loss: 1.6145 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.30198\n",
      "Epoch 54/1000\n",
      "2534/2534 [==============================] - 0s 171us/step - loss: 1.6144 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.30198\n",
      "Epoch 55/1000\n",
      "2534/2534 [==============================] - 1s 237us/step - loss: 1.6144 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.30198\n",
      "Epoch 56/1000\n",
      "2534/2534 [==============================] - 0s 182us/step - loss: 1.6143 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3035 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.30198\n",
      "Epoch 57/1000\n",
      "2534/2534 [==============================] - 1s 228us/step - loss: 1.6144 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3032 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss did not improve from 0.30198\n",
      "Epoch 58/1000\n",
      "2534/2534 [==============================] - 1s 234us/step - loss: 1.6143 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.30198\n",
      "Epoch 59/1000\n",
      "2534/2534 [==============================] - 0s 161us/step - loss: 1.6142 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.30198\n",
      "Epoch 60/1000\n",
      "2534/2534 [==============================] - 1s 219us/step - loss: 1.6143 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.30198\n",
      "Epoch 61/1000\n",
      "2534/2534 [==============================] - 0s 195us/step - loss: 1.6142 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3032 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.30198\n",
      "Epoch 62/1000\n",
      "2534/2534 [==============================] - 1s 226us/step - loss: 1.6141 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.30198\n",
      "Epoch 63/1000\n",
      "2534/2534 [==============================] - 1s 236us/step - loss: 1.6142 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.30198\n",
      "Epoch 64/1000\n",
      "2534/2534 [==============================] - 1s 204us/step - loss: 1.6139 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3032 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.30198\n",
      "Epoch 65/1000\n",
      "2534/2534 [==============================] - 0s 179us/step - loss: 1.6140 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3032 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.30198\n",
      "Epoch 66/1000\n",
      "2534/2534 [==============================] - 0s 189us/step - loss: 1.6139 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.30198\n",
      "Epoch 67/1000\n",
      "2534/2534 [==============================] - 0s 194us/step - loss: 1.6139 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3033 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.30198\n",
      "Epoch 68/1000\n",
      "2534/2534 [==============================] - 0s 130us/step - loss: 1.6142 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.30198\n",
      "Epoch 69/1000\n",
      "2534/2534 [==============================] - 0s 128us/step - loss: 1.6139 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.30198\n",
      "Epoch 70/1000\n",
      "2534/2534 [==============================] - 0s 174us/step - loss: 1.6136 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.30198\n",
      "Epoch 71/1000\n",
      "2534/2534 [==============================] - 0s 125us/step - loss: 1.6137 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.30198\n",
      "Epoch 72/1000\n",
      "2534/2534 [==============================] - 0s 128us/step - loss: 1.6138 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.30198\n",
      "Epoch 73/1000\n",
      "2534/2534 [==============================] - 0s 158us/step - loss: 1.6135 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.30198\n",
      "Epoch 74/1000\n",
      "2534/2534 [==============================] - 0s 175us/step - loss: 1.6136 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.30198\n",
      "Epoch 75/1000\n",
      "2534/2534 [==============================] - 0s 110us/step - loss: 1.6136 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3032 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.30198\n",
      "Epoch 76/1000\n",
      "2534/2534 [==============================] - 0s 149us/step - loss: 1.6134 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3034 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.30198\n",
      "Epoch 77/1000\n",
      "2534/2534 [==============================] - 0s 143us/step - loss: 1.6137 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5702 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.30198\n",
      "Epoch 78/1000\n",
      "2534/2534 [==============================] - 0s 163us/step - loss: 1.6135 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.30198\n",
      "Epoch 79/1000\n",
      "2534/2534 [==============================] - 0s 135us/step - loss: 1.6134 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.30198\n",
      "Epoch 80/1000\n",
      "2534/2534 [==============================] - 0s 119us/step - loss: 1.6131 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.30198\n",
      "Epoch 81/1000\n",
      "2534/2534 [==============================] - 0s 161us/step - loss: 1.6132 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.30198\n",
      "Epoch 82/1000\n",
      "2534/2534 [==============================] - 0s 190us/step - loss: 1.6133 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.30198\n",
      "Epoch 83/1000\n",
      "2534/2534 [==============================] - 0s 183us/step - loss: 1.6132 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.30198\n",
      "Epoch 84/1000\n",
      "2534/2534 [==============================] - 0s 182us/step - loss: 1.6129 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.30198\n",
      "Epoch 85/1000\n",
      "2534/2534 [==============================] - 0s 146us/step - loss: 1.6127 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.30198\n",
      "Epoch 86/1000\n",
      "2534/2534 [==============================] - 0s 116us/step - loss: 1.6126 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_loss did not improve from 0.30198\n",
      "Epoch 87/1000\n",
      "2534/2534 [==============================] - 0s 151us/step - loss: 1.6127 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.30198\n",
      "Epoch 88/1000\n",
      "2534/2534 [==============================] - 0s 164us/step - loss: 1.6129 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.30198\n",
      "Epoch 89/1000\n",
      "2534/2534 [==============================] - 0s 120us/step - loss: 1.6128 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.30198\n",
      "Epoch 90/1000\n",
      "2534/2534 [==============================] - 0s 141us/step - loss: 1.6125 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.30198\n",
      "Epoch 91/1000\n",
      "2534/2534 [==============================] - 0s 116us/step - loss: 1.6124 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3029 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.30198\n",
      "Epoch 92/1000\n",
      "2534/2534 [==============================] - 0s 141us/step - loss: 1.6129 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.30198\n",
      "Epoch 93/1000\n",
      "2534/2534 [==============================] - 0s 153us/step - loss: 1.6124 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.30198\n",
      "Epoch 94/1000\n",
      "2534/2534 [==============================] - 0s 103us/step - loss: 1.6124 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.30198\n",
      "Epoch 95/1000\n",
      "2534/2534 [==============================] - 0s 131us/step - loss: 1.6122 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.30198\n",
      "Epoch 96/1000\n",
      "2534/2534 [==============================] - 0s 122us/step - loss: 1.6121 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.30198\n",
      "Epoch 97/1000\n",
      "2534/2534 [==============================] - 0s 115us/step - loss: 1.6119 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.30198\n",
      "Epoch 98/1000\n",
      "2534/2534 [==============================] - 0s 109us/step - loss: 1.6124 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.30198\n",
      "Epoch 99/1000\n",
      "2534/2534 [==============================] - 0s 117us/step - loss: 1.6121 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.30198\n",
      "Epoch 100/1000\n",
      "2534/2534 [==============================] - 0s 111us/step - loss: 1.6120 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3031 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.30198\n",
      "Epoch 101/1000\n",
      "2534/2534 [==============================] - 0s 109us/step - loss: 1.6116 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3030 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.30198\n",
      "Epoch 102/1000\n",
      "2534/2534 [==============================] - 0s 111us/step - loss: 1.6116 - mean_squared_error: 0.9515 - mean_absolute_error: 0.5701 - val_loss: 0.3028 - val_mean_squared_error: 0.1855 - val_mean_absolute_error: 0.4041\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.30198\n",
      "Epoch 00102: early stopping\n"
     ]
    }
   ],
   "source": [
    "#step 5\n",
    "import numpy as np\n",
    "#Going through 10 different training set sizes and saving results to val_testsize\n",
    "val_testsize = {}\n",
    "\n",
    "#for exponent in np.linspace(-8,-3,10,endpoint=True):\n",
    "#keep for loop to run across all exponents\n",
    "##for exponent in np.linspace(.5,5,20,endpoint=True):\n",
    "    #print(test_size)\n",
    "    ##val_testsize[exponent] = testing_training_size(exponent*10**-6)\n",
    "    \n",
    "#testing one size - 4.289473684210526\n",
    "val_testsize[4.289] = testing_training_size(4.289473684210526*10**-6)\n",
    "#val_testsize[.2] = testing_training_size(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip if only using one exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponent:  4.289\n",
      "Final Val loss:  0.15978504206163427\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"Runs:\", len(val_testsize))\n",
    "#print(\"Test size:\", val_testsize.keys())\n",
    "#print(val_testsize[-8.0][-1])\n",
    "for key,val in val_testsize.items():\n",
    "    print(\"Exponent: \", key)\n",
    "    print(\"Final Val loss: \", val_testsize[key][-1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalloss_size = {}\n",
    "finalloss_size[6] = val_testsize[4.289]\n",
    "df_fl = pd.DataFrame.from_dict(finalloss_size, orient='index')\n",
    "#df_fl.reset_index().to_csv('ActiveLearn_Loss',index='index')\n",
    "##df_fl.to_csv('w_both_AL_500_Loss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at prediction loss on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4116, 116)\n",
      "(4116, 116)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "print(XV_val.shape)\n",
    "print(XP_val.shape)\n",
    "gaus_indices = np.load('hybridsort_saved_indices.npy')\n",
    "print(gaus_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNumber of applications in validations:\\n## Total 4116 ##\\nbackprop 4002\\nkmeans 4\\nsrad 1 \\nleukocyte 36\\nhybridsort 51\\ngaussian 15\\nstream 7\\n\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Number of applications in validations:\n",
    "## Total 4116 ##\n",
    "backprop 4002\n",
    "kmeans 4\n",
    "srad 1 \n",
    "leukocyte 36\n",
    "hybridsort 51\n",
    "gaussian 15\n",
    "stream 7\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model_10 = load_model('10perdata_dram_bias.h5',custom_objects={'loss': weighted_mse(loss_weight)})\n",
    "##model_10 = load_model('20perdata_dram_kmeans_zerobias.h5', custom_objects={'loss':weighted_mse(loss_weight)})\n",
    "model_10 = load_model('membounds_dram_kmeans_zerobias.h5', custom_objects={'loss':weighted_mse(loss_weight)})\n",
    "#model_10 = load_model('wsame_PV_dl_10per_woutbias0.h5')\n",
    "#model_10 = load_model('same_PV_dl_20percmodel116.h5')\n",
    "#model_10 = load_model('wsame_PV_dl_10per_400.h5',custom_objects={'loss': weighted_mse(loss_weight)})\n",
    "y_pred = model_10.predict(XP_val)\n",
    "X_testV = XV_val\n",
    "#y_pred = y_pred[gaus_indices][:,96] #96 = ipc\n",
    "#X_testV = X_testV[gaus_indices][:,96] #96 = ipc\n",
    "y_pred = y_pred[gaus_indices][:,34] #34 = dram_write_throughput\n",
    "X_testV = X_testV[gaus_indices][:,34] #34 = dram_write_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(column_name_dict['dram_write_throughput'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape\n",
    "#y_pred[:,96].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: -0.17474171916660441\n",
      "MAE: 0.0141403093903449\n",
      "RMSE: 0.026044769368398998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Averaging down the columns first (metrics), which converts it to 1 error per metric, then averages that (one row)\n",
    "mse = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "normX_testV = np.mean(np.mean(np.square(X_testV),axis=0))\n",
    "#relative to the full scale of all the test data - 3x bigger because it's one number scaling across all metrics\n",
    "mse_relative = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))/normX_testV\n",
    "mspe = np.mean(np.mean(np.square(y_pred - X_testV),axis=0))/normX_testV ## ask bethany\n",
    "rmse_relative = np.sqrt(mspe)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "print (\"R:\",  sp.stats.pearsonr(X_testV.flatten(), y_pred.flatten())[0])\n",
    "print (\"MAE:\", np.mean(np.abs(y_pred.flatten() - X_testV.flatten())))\n",
    "print (\"RMSE:\", np.sqrt(np.power(X_testV.flatten()- y_pred.flatten(), 2).mean()))\n",
    "\n",
    "#left out the mean, i would get the error rate for each metric\n",
    "#first show val error decreasings as training data increases\n",
    "#second show that val error can decreases earlier with active learning implementation\n",
    "\n",
    "#print(\"MSE:\", mse)\n",
    "#print(\"RMSE:\", rmse)\n",
    "#print(\"MSPE:\", mspe)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Make the plot\n",
    "ax.scatter(X_testV, y_pred, alpha=0.5)\n",
    "\n",
    "# Make it pretty\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "\n",
    "ax.set_xlabel('True Metrics')\n",
    "ax.set_ylabel('Pred. Metrics')\n",
    "#ax.set_ylim([-10,30])\n",
    "#ax.set_xlim([-10,30])\n",
    "ax.axis('equal')\n",
    "fig.set_size_inches(3.5, 3.5)\n",
    "plt.title('Results for hybridsort - weighted at 100 only - DRAM')\n",
    "# Add in the goal line\n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), 'k--');\n",
    "#plt.savefig('intra_arch_pred.png', bbox_inches='tight')\n",
    "#not being penalized from predicting value outside of the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dram_Write(x)read(y) for predict and true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[2]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_pred.shape)\n",
    "x = np.array([[1,2,3],[1,4,5]])\n",
    "print(x.shape)\n",
    "print(x[:,[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction analysis on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2059, 116)\n",
      "(2059,)\n",
      "(2059, 116)\n"
     ]
    }
   ],
   "source": [
    "#step 6 : need to make sure test data imported is correct or \n",
    "# if it's validation data set that needs to be imported?\n",
    "import numpy as np\n",
    "X_testV = np.load('XV_test_data_all_metrics.npy')\n",
    "X_testP = np.load('XP_test_data_all_metrics.npy')\n",
    "y_testV = np.load('y_test_data_all_metrics.npy')\n",
    "print(X_testV.shape)\n",
    "print(y_testV.shape)\n",
    "print(X_testP.shape)\n",
    "#need to import X_Test from V_100 metrics\n",
    "#need to compare predictions with true output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: 0.5312856394746882\n",
      "MAE: 0.18932281641601806\n",
      "RMSE: 0.2909430530667074\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#step 7: analysis of model performance\n",
    "from keras.models import load_model\n",
    "\n",
    "#p_model = load_model('PV_dl_20percmodel11620.0.h5')\n",
    "p_model = load_model('PV_dl_20percmodel1161_92.h5')\n",
    "y_pred = p_model.predict(X_testP)\n",
    "\n",
    "\n",
    "#Averaging down the columns first (metrics), which converts it to 1 error per metric, then averages that (one row)\n",
    "mse = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "normX_testV = np.mean(np.mean(np.square(X_testV),axis=0))\n",
    "#relative to the full scale of all the test data - 3x bigger because it's one number scaling across all metrics\n",
    "mse_relative = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))/normX_testV\n",
    "mspe = np.mean(np.mean(np.square(y_pred - X_testV),axis=0))/normX_testV ## ask bethany\n",
    "rmse_relative = np.sqrt(mspe)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "print (\"R:\",  sp.stats.pearsonr(X_testV.flatten(), y_pred.flatten())[0])\n",
    "print (\"MAE:\", np.mean(np.abs(y_pred.flatten() - X_testV.flatten())))\n",
    "print (\"RMSE:\", np.sqrt(np.power(X_testV.flatten()- y_pred.flatten(), 2).mean()))\n",
    "\n",
    "#left out the mean, i would get the error rate for each metric\n",
    "#first show val error decreasings as training data increases\n",
    "#second show that val error can decreases earlier with active learning implementation\n",
    "\n",
    "#print(\"MSE:\", mse)\n",
    "#print(\"RMSE:\", rmse)\n",
    "#print(\"MSPE:\", mspe)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Make the plot\n",
    "ax.scatter(X_testV, y_pred, alpha=0.5)\n",
    "\n",
    "# Make it pretty\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "\n",
    "ax.set_xlabel('True Metrics')\n",
    "ax.set_ylabel('Pred. Metrics')\n",
    "#ax.set_ylim([-10,30])\n",
    "#ax.set_xlim([-10,30])\n",
    "ax.axis('equal')\n",
    "fig.set_size_inches(3.5, 3.5)\n",
    "\n",
    "# Add in the goal line\n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), 'k--');\n",
    "#plt.savefig('intra_arch_pred.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning analysis\n",
    "Active learning model chooses data set dependent on variance or by random selection. The kernel names are used to pick data points from both P100 and V100 data sets to create new corresponding data sets. These corresponding data points are used to train the deep learning model above to predict all V100 metrics given P100 metrics at different percentages of training data amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used when changing training dataset percentage\n",
    "#previous method - multiple sizes within DL model\n",
    "#Getting final val_loss\n",
    "\n",
    "bar_nam = [.99,.89,.79,.69,.59,.49,.39,.29,.19,.09]\n",
    "import matplotlib.pyplot as plt\n",
    "finalloss_size = {}\n",
    "for k,v in val_testsize.items():\n",
    "    # get final loss for each test size\n",
    "    finalloss_size[k] = val_testsize[k][-1]\n",
    "    #print(k)\n",
    "#saving data\n",
    "df_fl = pd.DataFrame.from_dict(finalloss_size, orient='index')\n",
    "#df_fl.reset_index().to_csv('ActiveLearn_Loss',index='index')\n",
    "df_fl.to_csv('AL3_Loss.csv')\n",
    "#print(finalloss_size.keys())\n",
    "#print(list(finalloss_size.values()))\n",
    "plt.bar(range(len(finalloss_size)), list(finalloss_size.values()), align='center')\n",
    "#plt.xticks(range(len(finalloss_size)), list(finalloss_size.keys()))\n",
    "plt.xticks(range(len(finalloss_size)), bar_nam)\n",
    "plt.xlabel('Percentage of data used in Training dataset')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Loss of data compared to size of training set: Variance Test 2')\n",
    "# # for python 2.x:\n",
    "# plt.bar(range(len(D)), D.values(), align='center')  # python 2.x\n",
    "# plt.xticks(range(len(D)), D.keys())  # in python 2.x\n",
    "plt.ylim((0.005,1.25))\n",
    "plt.savefig(\"AL3_PV_loss_trainsize_2678.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting loss vs epoch of different training set sizes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print(history.history.keys())\n",
    "# \"Loss\"\n",
    "#cycle through entire dataset\n",
    "\"\"\"\n",
    "for k, v in val_testsize.items():\n",
    "    #print(k)\n",
    "    plt.plot(val_testsize[k])\n",
    "\"\"\"\n",
    "#plot singular run\n",
    "plt.plot(val_testsize[4.289473684210526])\n",
    "#plt.plot(val_testsize[0.01])\n",
    "#plt.plot(val_testsize[0.51])\n",
    "plt.title('Training data percentage Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['.99', '.89', '.79','.69','.59','.49','.39','.29','.19','.09'], loc='upper right')\n",
    "plt.savefig(\"PV_lossepoch_trainsize_2678.png\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results of models created above on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "p_model = load_model('dl_model0.11.h5')\n",
    "new_p = pd.read_csv('cor_p100.csv', index_col = 0)\n",
    "new_v = pd.read_csv('cor_v100.csv', index_col = 0)\n",
    "print(new_p.shape)\n",
    "print(new_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainP, X_testP, y_trainP, y_testP, X_P, Y_P = process(new_p,test_size)\n",
    "X_trainV, X_testV, y_trainV, y_testV, X_V, Y_V = process(new_v,test_size)\n",
    "print(X_testP.shape)\n",
    "print(y_testV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred = p_model.predict(X_testP)\n",
    "#print(y_pred[1])\n",
    "#print(X_testP[1])\n",
    "#give it a seed \n",
    "\n",
    "#Averaging down the columns first (metrics), which converts it to 1 error per metric, then averages that (one row)\n",
    "mse = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "normX_testV = np.mean(np.mean(np.square(y_testV),axis=0))\n",
    "#relative to the full scale of all the test data - 3x bigger because it's one number scaling across all metrics\n",
    "mse_relative = np.mean(np.mean(np.square(y_pred-X_testV),axis=0))/normX_testV\n",
    "mspe = np.mean(np.mean(np.square(y_pred - X_testV),axis=0))/normX_testV ## ask bethany\n",
    "rmse_relative = np.sqrt(mspe)\n",
    "\n",
    "#left out the mean, i would get the error rate for each metric\n",
    "#first show val error decreasings as training data increases\n",
    "#second show that val error can decreases earlier with active learning implementation\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MSPE:\", mspe)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Make the plot\n",
    "ax.scatter(X_testV, y_pred, alpha=0.5)\n",
    "\n",
    "# Make it pretty\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "\n",
    "ax.set_xlabel('True Metrics')\n",
    "ax.set_ylabel('Pred. Metrics')\n",
    "#ax.set_ylim([-10,30])\n",
    "#ax.set_xlim([-10,30])\n",
    "ax.axis('equal')\n",
    "fig.set_size_inches(3.5, 3.5)\n",
    "\n",
    "# Add in the goal line\n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), 'k--');\n",
    "#plt.savefig('intra_arch_pred.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Plotting predicted vs true\n",
    "print (\"R:\",  sp.stats.pearsonr(X_testV.flatten(), y_pred.flatten())[0])\n",
    "print (\"MAE:\", np.abs(y_pred.flatten() - X_testV.flatten()) , 's')\n",
    "print (\"RMSE:\", np.sqrt(np.power(X_testV.flatten()- y_pred.flatten(), 2).mean()), 's')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Make the plot\n",
    "ax.scatter(X_testV, y_pred, alpha=0.5)\n",
    "\n",
    "# Make it pretty\n",
    "ax.set_xlim(ax.get_xlim())\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "\n",
    "ax.set_xlabel('True Metrics')\n",
    "ax.set_ylabel('Pred. Metrics')\n",
    "#ax.set_ylim([-10,30])\n",
    "#ax.set_xlim([-10,30])\n",
    "ax.axis('equal')\n",
    "fig.set_size_inches(3.5, 3.5)\n",
    "\n",
    "# Add in the goal line\n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), 'k--');\n",
    "plt.savefig(\"metric_predict.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing RMSE for all Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 116\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, np.sqrt(np.mean(np.square(y_pred -X_testV),axis=0)), width)\n",
    "\n",
    "#Showing which metrics are more difficult to acquire\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE of 116 Metrics')\n",
    "\n",
    "plt.savefig('RMSE_metrics.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63   1  59 103  29  99  46  32   2 100]\n",
      "Top most difficult to predict metrics: ['stall_memory_dependency' 'branch_efficiency' 'dram_utilization'\n",
      " 'l2_utilization' 'local_memory_overhead' 'sm_efficiency' 'gst_efficiency'\n",
      " 'l2_tex_write_hit_rate' 'warp_execution_efficiency' 'achieved_occupancy']\n",
      "IPC metric number 96\n",
      "IPC RMSE value: 0.37079071177670936\n",
      "Stall memory dependency RMSE: 0.6104020683441777\n",
      "Memory Bandwidth 33\n",
      "RMSE Memory Bandwidth 0.28468231052347975\n",
      "0.9354361222246397\n",
      "l2_tex_write_hit_rate\n"
     ]
    }
   ],
   "source": [
    "metric_results = np.sqrt(np.mean(np.square(y_pred -X_testV),axis=0))\n",
    "ind = np.argpartition(metric_results, -10)[-10:]\n",
    "print(ind)\n",
    "print('Top most difficult to predict metrics:', XP_column_names[ind])\n",
    "print(\"IPC metric number\", column_name_dict['ipc'])\n",
    "print(\"IPC RMSE value:\", metric_results[96])\n",
    "print('Stall memory dependency RMSE:', metric_results[column_name_dict['stall_memory_dependency']])\n",
    "print(\"Memory Bandwidth\", column_name_dict['dram_read_throughput'])\n",
    "print(\"RMSE Memory Bandwidth\", metric_results[33])\n",
    "print(metric_results[32])\n",
    "print(XP_column_names[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_vals = np.sqrt(np.mean(np.square(y_pred -X_testV),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_model = my_model()\n",
    "history = h_model.fit(X_trainP, X_trainV, epochs=200, batch_size=10000,  verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of different metrics\n",
    "#RMSE shows that there is around a 1% error in predicting V100 metrics from P100 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
