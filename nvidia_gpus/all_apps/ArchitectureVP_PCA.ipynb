{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going through all single runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going through metrics\n",
    "import numbers\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if True:\n",
    "    combined_data_ = {}\n",
    "    target_kernels = []\n",
    "    \n",
    "metric_targets = []\n",
    "all_sig_metrics = []\n",
    "bench_targets = [ \"gaussianP\" , \"gaussianV\", \"gemm\", \"stream\", \"kmeans_cudaV\", \"kmeans_cudaP\" ]\n",
    "#for filen_ in glob.glob(\"/home/yzamora/power/nvidia_gpus/all_apps/mixed_results/*.csv\"):\n",
    "#for filen_ in glob.glob(\"/home/yzamora/power/nvidia_gpus/all_apps/gaussian_results/*.csv\"):\n",
    "for filen_ in glob.glob(\"/gpfs/jlse-fs0/users/yzamora/v100_testing/v100_p100_results/*.csv\"):\n",
    "    filen = os.path.basename(filen_)\n",
    "    #print (filen)\n",
    "    filen_split = filen.split('.')[0].split('_')\n",
    "    bench_name = filen_split[0]\n",
    "    size = filen_split[1]\n",
    "    #metric_name = filen_split[1]\n",
    "    #print(filen_split[0][-1]) #gives you p or v\n",
    "    #size_str = filen_split[1].split('N')[1]\n",
    "    \n",
    "    #if not (metric_name in metric_targets): continue\n",
    "    #if not (bench_name in bench_targets): continue\n",
    "    \n",
    "    key_root = bench_name\n",
    "    #print(key_root)\n",
    "    levels = [\"Idle\", \"Low\",\"High\", \"Max\"]\n",
    "    bw_units = [\"GB\", \"MB\", \"KB\" ,\"0B\"]\n",
    "    # Now open the file and look for the data\n",
    "    with open(filen_ ,'r') as file_handle:\n",
    "        #print (file_handle)\n",
    "        data_found = False\n",
    "        ncols = 1\n",
    "        fdata = csv.reader(file_handle)\n",
    "        index_lookup = {}\n",
    "        #print(filen_)\n",
    "        for line_split in fdata:\n",
    "            #print (line_split)\n",
    "            lsplt = (len(line_split) > 0)\n",
    "      \n",
    "            if data_found:\n",
    "                #print(\"data found\")\n",
    "                if lsplt and len(line_split) == ncols:\n",
    "                    #percent - strip off end\n",
    "                    # Get metric name here\n",
    "                    #mname_index = index_lookup['Metric Name']\n",
    "                    #metric_name = line_split[ mname_index ]\n",
    "                    #if not (metric_name in metric_targets): continue\n",
    "                    \n",
    "                    # Read in desired value for the current metric\n",
    "                    target_index = index_lookup['Avg']; value = 0\n",
    "                    metric_name = line_split[index_lookup['Metric Name']]\n",
    "                    #print (line_split[target_index].isdecimal())\n",
    "                    if line_split[target_index].isdecimal():\n",
    "                        if line_split[target_index]!= '0':\n",
    "                            #print(line_split[target_index])\n",
    "                            all_sig_metrics.append(metric_name)\n",
    "                            value = int(line_split[ target_index ])\n",
    "                        \n",
    "                        # Labeled with percentage\n",
    "                    elif \"%\" == line_split[target_index][-1]:\n",
    "                        #print (\"percentage loop\")\n",
    "                        all_sig_metrics.append(metric_name)\n",
    "                        value = float(line_split[ target_index ][0:7]) / 100.0\n",
    "                        \n",
    "                    # Labeled with bandwidth units\n",
    "                    elif line_split[ target_index ][-4:-2] in bw_units:\n",
    "                        # Just take the first \n",
    "                        units = line_split[ target_index ][-4:-2]\n",
    "                        all_sig_metrics.append(metric_name)\n",
    "                        mfact = 1.0\n",
    "                        if   units == \"KB\": mfact = 1024\n",
    "                        elif units == \"MB\": mfact = 1024*1024\n",
    "                        elif units == \"GB\": mfact = 1024*1024*1024\n",
    "                        elif units == \"0B\":  mfact = 1\n",
    "                        value = float(line_split[ target_index ][0:7]) * mfact\n",
    "                    \n",
    "                    # idle, low, max\n",
    "                    elif line_split[ target_index ][-1] == \")\":\n",
    "                        #print (\"low\")\n",
    "                        all_sig_metrics.append(metric_name)\n",
    "                        value = int(line_split[ target_index].split('(')[1].split(\")\")[0])\n",
    "                        \n",
    "                    # otherwise, float\n",
    "                    #elif not(float(line_split[ target_index ]).is_integer()):\n",
    "                    else:\n",
    "                        #print(line_split[ target_index ].split('(')[0])\n",
    "                        #print(\"in float\")\n",
    "                        #print(line_split[ target_index ].split('(')[0])\n",
    "                        value = float(line_split[ target_index ])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                     # Parse name of kernel\n",
    "                    kernel_name = line_split[ index_lookup['Kernel'] ].split('(')[0]\n",
    "                    # Define kernel-specific key\n",
    "                    key = key_root + size + \"_\" + kernel_name \n",
    "                    \n",
    "                    # Initialize dict for this key, if it is new\n",
    "                    if not (key in combined_data_): \n",
    "                        combined_data_ [ key ] = {}\n",
    "                    if not (kernel_name in target_kernels):\n",
    "                        target_kernels.append(kernel_name)\n",
    "                        #combined_data_ [ key ][ 'size' ] = int( size_str )\n",
    "                        \n",
    "                    # Store value for the metric being read right now\n",
    "                    combined_data_ [key][ metric_name ] = value\n",
    "                    combined_data_[key][\"kernelname\"] = kernel_name\n",
    "                    \n",
    "                else: data_found = False\n",
    "\n",
    "\n",
    "            elif lsplt and line_split[0] == 'Device' and line_split[1] == 'Kernel':\n",
    "                # Set flag that we are at the data:\n",
    "                data_found = True\n",
    "                # Set number of columns in table:\n",
    "                ncols = len(line_split)\n",
    "                # Generate an index lookup table:\n",
    "                idx = 0\n",
    "                for term in line_split:\n",
    "                    index_lookup[term] = idx\n",
    "                    idx += 1\n",
    "                #print(index_lookup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gaussianPN1088_Fan2', 'gaussianPN1088_Fan1', 'gaussianPN1376_Fan2', 'gaussianPN1376_Fan1', 'gaussianPN448_Fan2', 'gaussianPN448_Fan1', 'kmeanscudaP_invert_mapping', 'kmeanscudaP_kmeansPoint', 'gaussianPN1568_Fan2', 'gaussianPN1568_Fan1', 'gaussianVN960_Fan2', 'gaussianVN960_Fan1', 'gaussianPN928_Fan2', 'gaussianPN928_Fan1', 'gaussianPN1024_Fan2', 'gaussianPN1024_Fan1', 'gaussianPN544_Fan2', 'gaussianPN544_Fan1', 'gaussianPN352_Fan2', 'gaussianPN352_Fan1', 'gaussianPN1248_Fan2', 'gaussianPN1248_Fan1', 'gaussianPN64_Fan2', 'gaussianPN64_Fan1', 'gaussianVN64_Fan2', 'gaussianVN64_Fan1', 'gaussianVN608_Fan2', 'gaussianVN608_Fan1', 'gaussianVN256_Fan2', 'gaussianVN256_Fan1', 'gaussianVN160_Fan2', 'gaussianVN160_Fan1', 'kmeanscudaV_invert_mapping', 'kmeanscudaV_kmeansPoint', 'gaussianPN1472_Fan2', 'gaussianPN1472_Fan1', 'gaussianPN992_Fan2', 'gaussianPN992_Fan1', 'gaussianPN384_Fan2', 'gaussianPN384_Fan1', 'gaussianVN96_Fan2', 'gaussianVN96_Fan1', 'gaussianPN480_Fan2', 'gaussianPN480_Fan1', 'gaussianVN544_Fan2', 'gaussianVN544_Fan1', 'gaussianPN512_Fan2', 'gaussianPN512_Fan1', 'gaussianPN2176_Fan2', 'gaussianPN2176_Fan1', 'gaussianVN128_Fan2', 'gaussianVN128_Fan1', 'gaussianVN288_Fan2', 'gaussianVN288_Fan1', 'gaussianVN736_Fan2', 'gaussianVN736_Fan1', 'gaussianVN512_Fan2', 'gaussianVN512_Fan1', 'gaussianPN1504_Fan2', 'gaussianPN1504_Fan1', 'gaussianVN416_Fan2', 'gaussianVN416_Fan1', 'gaussianPN320_Fan2', 'gaussianPN320_Fan1', 'gaussianVN704_Fan2', 'gaussianVN704_Fan1', 'gaussianPN832_Fan2', 'gaussianPN832_Fan1', 'gaussianPN1664_Fan2', 'gaussianPN1664_Fan1', 'gaussianVN352_Fan2', 'gaussianVN352_Fan1', 'gaussianPN288_Fan2', 'gaussianPN288_Fan1', 'gaussianVN576_Fan2', 'gaussianVN576_Fan1', 'gaussianVN928_Fan2', 'gaussianVN928_Fan1', 'gaussianPN960_Fan2', 'gaussianPN960_Fan1', 'gaussianPN1056_Fan2', 'gaussianPN1056_Fan1', 'gaussianPN640_Fan2', 'gaussianPN640_Fan1', 'gaussianPN896_Fan2', 'gaussianPN896_Fan1', 'gaussianVN864_Fan2', 'gaussianVN864_Fan1', 'gaussianPN1440_Fan2', 'gaussianPN1440_Fan1', 'gaussianPN160_Fan2', 'gaussianPN160_Fan1', 'gaussianPN1216_Fan2', 'gaussianPN1216_Fan1', 'gaussianPN1408_Fan2', 'gaussianPN1408_Fan1', 'gaussianVN224_Fan2', 'gaussianVN224_Fan1', 'gaussianPN96_Fan2', 'gaussianPN96_Fan1', 'gaussianPN1184_Fan2', 'gaussianPN1184_Fan1', 'gaussianPN608_Fan2', 'gaussianPN608_Fan1', 'gaussianPN1792_Fan2', 'gaussianPN1792_Fan1', 'gaussianPN2048_Fan2', 'gaussianPN2048_Fan1', 'gaussianVN480_Fan2', 'gaussianVN480_Fan1', 'gaussianVN384_Fan2', 'gaussianVN384_Fan1', 'gaussianPN2560_Fan2', 'gaussianPN2560_Fan1', 'gaussianPN736_Fan2', 'gaussianPN736_Fan1', 'gaussianPN416_Fan2', 'gaussianPN416_Fan1', 'gaussianVN832_Fan2', 'gaussianVN832_Fan1', 'gaussianPN1280_Fan2', 'gaussianPN1280_Fan1', 'gaussianVN192_Fan2', 'gaussianVN192_Fan1', 'gaussianPN1632_Fan2', 'gaussianPN1632_Fan1', 'gaussianPN768_Fan2', 'gaussianPN768_Fan1', 'gaussianPN128_Fan2', 'gaussianPN128_Fan1', 'gaussianVN800_Fan2', 'gaussianVN800_Fan1', 'gaussianPN2304_Fan2', 'gaussianPN2304_Fan1', 'gaussianVN768_Fan2', 'gaussianVN768_Fan1', 'gaussianPN1344_Fan2', 'gaussianPN1344_Fan1', 'gaussianPN224_Fan2', 'gaussianPN224_Fan1', 'gaussianPN1920_Fan2', 'gaussianPN1920_Fan1', 'gaussianPN256_Fan2', 'gaussianPN256_Fan1', 'gaussianVN448_Fan2', 'gaussianVN448_Fan1', 'gaussianPN576_Fan2', 'gaussianPN576_Fan1', 'gaussianPN32_Fan2', 'gaussianPN32_Fan1', 'gaussianVN896_Fan2', 'gaussianVN896_Fan1', 'gaussianPN1600_Fan2', 'gaussianPN1600_Fan1', 'gaussianPN192_Fan2', 'gaussianPN192_Fan1', 'gaussianPN800_Fan2', 'gaussianPN800_Fan1', 'gaussianPN1312_Fan2', 'gaussianPN1312_Fan1', 'gaussianPN672_Fan2', 'gaussianPN672_Fan1', 'gaussianVN672_Fan2', 'gaussianVN672_Fan1', 'gaussianPN1120_Fan2', 'gaussianPN1120_Fan1', 'gaussianPN1536_Fan2', 'gaussianPN1536_Fan1', 'gaussianPN864_Fan2', 'gaussianPN864_Fan1', 'gaussianPN704_Fan2', 'gaussianPN704_Fan1', 'gaussianVN320_Fan2', 'gaussianVN320_Fan1', 'gaussianPN1152_Fan2', 'gaussianPN1152_Fan1', 'gaussianVN640_Fan2', 'gaussianVN640_Fan1', 'gaussianPN2432_Fan2', 'gaussianPN2432_Fan1'])\n"
     ]
    }
   ],
   "source": [
    "#print(combined_data_['']['kernalname'])\n",
    "#print(combined_data_['gaussianPN256_Fan2'])\n",
    "#print(target_kernels)\n",
    "#target_kernels = ['Fan2p', 'Fan1p','Fan2v', 'Fan1v']\n",
    "#print(target_kernels)\n",
    "for key,value in combined_data_.items():\n",
    "    if 'P' in key:\n",
    "        combined_data_[key]['Architecture'] = 'P100'\n",
    "    else:\n",
    "        combined_data_[key]['Architecture'] = 'V100'\n",
    "#print(combined_data_['gaussianPN256_Fan2']['Architecture'])\n",
    "print(combined_data_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame.from_dict(combined_data_,orient='index')\n",
    "#print(df.shape)\n",
    "#print(df.columns)\n",
    "#Num_cols = len (df.columns)\n",
    "#print(Num_cols)\n",
    "#print(scale(df))\n",
    "#print((df['kernelname'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['inst_per_warp', 'kernelname', 'branch_efficiency',\n",
      "       'warp_execution_efficiency', 'warp_nonpred_execution_efficiency',\n",
      "       'inst_replay_overhead', 'shared_load_transactions_per_request',\n",
      "       'shared_store_transactions_per_request',\n",
      "       'local_load_transactions_per_request',\n",
      "       'local_store_transactions_per_request',\n",
      "       ...\n",
      "       'nvlink_user_ratom_data_transmitted',\n",
      "       'nvlink_total_write_data_transmitted',\n",
      "       'nvlink_user_write_data_transmitted', 'nvlink_transmit_throughput',\n",
      "       'nvlink_receive_throughput', 'nvlink_total_response_data_received',\n",
      "       'nvlink_user_response_data_received',\n",
      "       'nvlink_data_transmission_efficiency', 'nvlink_data_receive_efficiency',\n",
      "       'stall_sleeping'],\n",
      "      dtype='object', length=181)\n",
      "(180, 120)\n"
     ]
    }
   ],
   "source": [
    "#df = df.T\n",
    "print(df.keys())\n",
    "df=df.dropna(axis=1,how='any')\n",
    "#df = df.dropna()\n",
    "print(df.shape)\n",
    "#print(df['gaussianPN1024_Fan1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "\n",
    "all_colors = []\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "for name, color in colors.items():\n",
    "    if len(name) == 1:\n",
    "        all_colors.append(name + \"o\")\n",
    "    else:\n",
    "        all_colors.append(name)\n",
    "#print(all_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'P100'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2e572fd389c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#digits = load_digits()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#data = scale(digits.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    131\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    132\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'P100'"
     ]
    }
   ],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "\n",
    "#np.random.seed(42)\n",
    "labels = df['kernelname'].tolist()\n",
    "#labels = df['Architecture'].tolist()\n",
    "\n",
    "#print(n_samples, n_features)\n",
    "#df2 = df.drop(columns=['Architecture'])\n",
    "df2 = df.drop(columns=['kernelname'])\n",
    "df2_values = df2.values\n",
    "\n",
    "#mask = np.any(np.isnan(df2) | np.equal(df2,0), axis=1)\n",
    "#df2[~mask]\n",
    "#print(df2)\n",
    "#print(df2)\n",
    "if True:\n",
    "    #digits = load_digits()\n",
    "    #data = scale(digits.data)\n",
    "    data = scale(df2.values)\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    n_digits = len(target_kernels)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    digits = load_digits()\n",
    "    data = scale(digits.data)\n",
    "\n",
    "    n_samples, n_features = data.shape\n",
    "    n_digits = len(np.unique(digits.target))\n",
    "    labels = digits.target\n",
    "\n",
    "    print(\"digits\", digits)\n",
    "    print(\"data\", data)\n",
    "    print(\"n_samples\", n_samples)\n",
    "    print(\"labels\",labels)\n",
    "    \"\"\"\n",
    "\n",
    "    sample_size = 300\n",
    "\n",
    "    print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "          % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "    print(82 * '_')\n",
    "    print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "\n",
    "    def bench_k_means(estimator, name, data):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=sample_size)))\n",
    "\n",
    "    bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "                  name=\"k-means++\", data=data)\n",
    "\n",
    "    bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "                  name=\"random\", data=data)\n",
    "\n",
    "    # in this case the seeding of the centers is deterministic, hence we run the\n",
    "    # kmeans algorithm only once with n_init=1\n",
    "    pca = PCA(n_components=n_digits).fit(data)\n",
    "    bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "                  name=\"PCA-based\",\n",
    "                  data=data)\n",
    "    print(82 * '_')\n",
    "\n",
    "    # #############################################################################\n",
    "    # Visualize the results on PCA-reduced data\n",
    "\n",
    "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "    kmeans.fit(reduced_data)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    #plt.figure(1)\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.clf()\n",
    "    \"\"\"\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.',markersize=2)\n",
    "    x =[]\n",
    "    y =[]\n",
    "    plot_dict = {}\n",
    "    for i in range(len(labels)):    \n",
    "        if labels[i] in plot_dict:\n",
    "            plot_dict[ labels[i] ][0].append( reduced_data[i, 0] )\n",
    "            plot_dict[ labels[i] ][1].append( reduced_data[i, 1] )\n",
    "        else:\n",
    "            plot_dict[ labels[i] ] = [ ]\n",
    "            plot_dict[ labels[i] ].append( [reduced_data[i, 0]] )\n",
    "            plot_dict[ labels[i] ].append( [reduced_data[i, 1]] )\n",
    "            #'bo', 'go', 'ro', 'co', 'mo', 'yo', 'ko', 'wo',\n",
    "    clr = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w','aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', \n",
    "           'bisque', 'black', 'blanchedalmond', 'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate'\n",
    "           , 'coral', 'cornflowerblue', 'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', \n",
    "           'darkgreen', 'darkgrey', 'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', \n",
    "           'darksalmon', 'darkseagreen', 'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', \n",
    "           'deeppink', 'deepskyblue', 'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', \n",
    "           'gainsboro', 'ghostwhite', 'gold', 'goldenrod', 'gray', 'green', 'greenyellow', 'grey', 'honeydew', 'hotpink', \n",
    "           'indianred', 'indigo', 'ivory', 'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue',\n",
    "           'lightcoral', 'lightcyan', 'lightgoldenrodyellow', 'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray', 'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine', 'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise', 'mediumvioletred', 'midnightblue',\n",
    "           'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive', 'olivedrab', 'orange', 'orangered', \n",
    "           'orchid']\n",
    "    #clr = [ 'bo', 'ko', 'go', 'co', 'yo', 'ro', 'bx', 'kx', 'gx', 'cx', 'yx', 'rx']\n",
    "    cind=0\n",
    "    for k,v in plot_dict.items():\n",
    "        plt.plot(v[0], v[1], color = clr[cind], label=k,  markersize=10, marker ='o', linestyle = 'None' )\n",
    "        cind+=1\n",
    "\n",
    "    \"\"\"\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\"\"\"\n",
    "    #plt.title('K-means clustering Application benchmarking data (PCA-reduced data)\\n'\n",
    "              #'Centroids are marked with white cross')\n",
    "    plt.title(\"PCA of Application Gaussian benchmarking data\",fontsize=24 )\n",
    "    plt.xlim(x_min, 10)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.ylabel(\"PC1\")\n",
    "    plt.xlabel(\"PC2\")\n",
    "    #plt.xticks(())\n",
    "    #plt.yticks(())\n",
    "\n",
    "    plt.legend(loc='best',prop={'size':14})\n",
    "    plt.savefig(\"PCA\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting the 118 dimensional space to a 2-d space where the data points belong to the targets.\n",
    "#principal components of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
